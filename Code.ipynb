{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing Positional Index"
      ],
      "metadata": {
        "id": "e-ngMmi3FlsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx pypdf2 pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXWUkJ75P1lw",
        "outputId": "8e5c7404-b9db-41db-d8fe-74b4b8e6a0a2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: pypdf2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]  # Remove stopwords and punctuation\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]  # Lemmatization\n",
        "    return lemmatized_tokens\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def read_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def read_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "def read_xlsx(file_path):\n",
        "    df = pd.read_excel(file_path, sheet_name=None)\n",
        "    text = \"\"\n",
        "    for sheet_name, sheet_df in df.items():\n",
        "        text += sheet_df.to_string(index=False, header=False)\n",
        "    return text\n",
        "\n",
        "def preprocess_document(doc_path):\n",
        "    _, ext = os.path.splitext(doc_path)\n",
        "    if ext == '.txt':\n",
        "        text = read_txt(doc_path)\n",
        "    elif ext == '.pdf':\n",
        "        text = read_pdf(doc_path)\n",
        "    elif ext == '.docx':\n",
        "        text = read_docx(doc_path)\n",
        "    elif ext == '.xlsx':\n",
        "        text = read_xlsx(doc_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
        "    return preprocess_text(text)\n",
        "\n",
        "def preprocess_documents(documents_path):\n",
        "    preprocessed_docs = {}\n",
        "    for filename in os.listdir(documents_path):\n",
        "        if filename.endswith(('.txt', '.pdf', '.docx', '.xlsx')):\n",
        "            doc_path = os.path.join(documents_path, filename)\n",
        "            doc_id = os.path.basename(doc_path)\n",
        "            preprocessed_docs[doc_id] = preprocess_document(doc_path)\n",
        "    return preprocessed_docs\n",
        "\n",
        "# Example usage\n",
        "documents_path = '/content/Documents'  # Update this path to your directory\n",
        "preprocessed_docs = preprocess_documents(documents_path)\n",
        "\n",
        "for doc_id, tokens in preprocessed_docs.items():\n",
        "    print(f\"Preprocessed {doc_id}: {tokens}\")\n",
        "\n",
        "def construct_inverted_index(documents_path):\n",
        "    inverted_index = {}\n",
        "    doc_id_mapping = {}\n",
        "    doc_id_counter = 0\n",
        "\n",
        "    for filename in os.listdir(documents_path):\n",
        "        if filename.endswith(('.txt', '.pdf', '.docx', '.xlsx')):\n",
        "            doc_id_counter += 1\n",
        "            doc_id = f\"doc_{doc_id_counter}\"\n",
        "            doc_id_mapping[doc_id] = filename\n",
        "            doc_path = os.path.join(documents_path, filename)\n",
        "            tokens = preprocess_document(doc_path)\n",
        "\n",
        "            for position, token in enumerate(tokens):\n",
        "                if token not in inverted_index:\n",
        "                    inverted_index[token] = {}\n",
        "                if doc_id not in inverted_index[token]:\n",
        "                    inverted_index[token][doc_id] = []\n",
        "                inverted_index[token][doc_id].append(position)\n",
        "\n",
        "    return inverted_index, doc_id_mapping\n",
        "\n",
        "# Example usage\n",
        "documents_path = '/content/Documents'  # Update this path to your directory\n",
        "inverted_index, doc_id_mapping = construct_inverted_index(documents_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi0cGq12FpiS",
        "outputId": "fa818be6-4e07-44e7-e24e-b47ab405c4d9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed dd.pdf: ['big', 'data', 'system', 'session', '7-distributed', 'programming', 'janardhanan', 'p', 'janardhanan.ps', 'wilp.bits', '-pilani.ac.intopics', 'today', '•top', 'design', '•types', 'parallelism', '•mapreduce', 'programming', 'model', '•see', 'map', 'reduce', 'program', 'work', 'using', 'hadoop', '•iterative', 'mapreduce', '•hands', 'demo', 'k', '-means', 'clustering', 'using', 'iterative', 'mapreduce', '2top', 'design', '-sequential', 'context', '•in', 'context', 'sequential', 'program', '•divide', 'conquer', '•it', 'easier', 'divide', 'problem', 'sub', '-problems', 'execute', 'one', 'one', '•a', 'sub', '-problem', 'definition', 'may', 'left', 'programmer', 'sequential', 'programming', 'context', '3f1', 'f2', 'f3', 'f4', 'main', 'f5', 'top', 'design', '-parallel', 'context', '•in', 'context', 'parallel', 'program', 'w', 'e', 'decompose', 'problem', 'sub', '-problems', 'anyway', 'programmer', 'chooses', '•need', 'think', '•each', 'sub', '-problem', 'need', 'assigned', 'processor', '•goal', 'get', 'program', 'work', 'faster', '•divide', 'problem', 'combine', 'end', 'final', 'answer', '•need', 'decide', 'combination', '•is', 'parallelism', 'combination', 'sequential', 'trivial', '4f1', 'f2', 'f2', 'f4', 'main', 'f5', 'f5', 'f7', 'f5', 'deciding', 'number', 'sub', '-problems', '•in', 'conventional', 'top', 'design', 'sequential', 'system', '•keep', 'number', 'sub', '-problems', 'manageable', '•because', 'need', 'keep', 'track', 'computation', 'progress', '•in', 'parallel', 'system', 'dictated', 'number', 'processor', '•processor', 'utilisation', 'key', '•if', 'n', 'processor', 'potentially', 'n', 'sub', 'problem', '5f1', 'f2', 'f2', 'f4', 'main', 'f5', 'f5', 'f7', 'f5', 'many', 'processor', 'top-down', 'design', '•at', 'level', 'problem', 'need', 'run', 'parallel', '6', 'example', '1', '-keyword', 'search', 'list', '•problem', '•search', 'key', 'k', 'sorted', 'list', 'l', 'size', 'n', '•data', '•ls', 'stored', 'distributed', 'system', 'p', 'processor', 'storing', 'n/p', 'item', '•solution', '•run', 'binary', 'search', 'p', 'processor', 'parallel', '•whichever', 'processor', 'find', 'k', 'return', 'j', 'ithprocessor', 'found', 'key', 'jthposition', '•combination', 'one', 'position', 'collected', 'processor', '0', '•speedup', 'p', '•time', 'complexity', 'log', 'n/p', '7k', 'p1', 'p2', 'p3', 'p4n/p', 'itemsexample', '2', '-fingerprint', 'matching', '•find', 'match', 'fingerprint', 'f', 'database', 'print', '•set', 'print', 'partitioned', 'evenly', 'stored', 'distributed', 'database', '•partitioning', 'infrequent', 'activity', '-only', 'many', 'new', 'entry', 'database', '•search', 'frequent', 'activity', '•speed', 'p', '•time', 'complexity', 'n/p', 'given', 'sequential', 'search', 'every', 'partition', '8', 'example', '3', 'document', 'search', '•find', 'keywords', 'document', 'distributed', 'document', 'collection', '9', 'topic', 'today', '•top', 'design', '•types', 'parallelism', '•mapreduce', 'programming', 'model', '•see', 'map', 'reduce', 'program', 'work', 'using', 'hadoop', '•iterative', 'mapreduce', '10types', 'parallelism', '•data', 'parallelism', '•tree', 'parallelism', '•task', 'parallelism', '•request', 'parallelism', '11data', 'parallel', 'execution', 'model', '•data', 'partitioned', 'multiple', 'node', 'processor', '•try', 'make', 'partition', 'equal', 'balanced', '•all', 'processor', 'execute', 'code', 'parallel', '•for', 'homogenous', 'node', 'equal', 'amount', 'work', 'utilization', 'close', '100', '•execution', 'time', 'overhead', 'minimal', '•unbalanced', 'data', 'size', 'work', 'heterogenous', 'node', 'lead', 'higher', 'execution', 'time', '12where', 'data', 'parallelism', 'possible', '•there', 'problem', 'divide', 'work', '1.equally', '2.independently', 'proceed', 'parallel', '•quicksort', 'l', 'n', '•all', 'n', 'item', 'l', 'memory', 'processor', '0', '•time', 'complexity', '•best', 'case', '-o', 'n.log', 'n', '•worst', 'case', '–o', 'n2', '13sorting', 'bubble', 'selection', 'insertion', 'merge', 'quick', 'counting', 'radix', '-visualgotree', 'parallelism', 'summary', '•dynamic', 'version', 'divide', 'conquer', '-partitions', 'done', 'dynamically', '•division', 'problem', 'sub', '-problems', 'happens', 'execution', 'time', '•sub-problem', 'identical', 'structure', 'larger', 'problem', '•what', 'division', 'step', '•in', 'quick', 'sort', 'picking', 'split', '2', 'sub', '-problems', '•division', 'partitioning', 'logic', 'important', 'find', 'almost', 'equal', 'sub', '-problems', '•if', 'problem', 'divided', 'k', 'sub', '-problems', '•then', 'log', 'knsteps', 'needed', 'n', 'processor', 'execute', 'parallel', '•if', 'p', 'n', 'work', 'get', 'done', 'log', 'n', 'time', 'list', 'item', 'assign', 'one', 'processor', 'finally', '•what', 'assign', 'p', 'processor', '•p', 'n', 'processor', 'utilised', '•p', 'n', '-utilised', 'processor', '14task', 'parallelism', '-example', '1', 'word', 'processor', '•parallel', 'task', 'work', 'data', '•unlike', 'data', 'tree', 'parallel', 'data', '’', 'need', 'divided', 'task', 'get', 'divided', 'sub', 'task', '•may', 'work', 'data', 'instance', 'else', 'need', 'make', 'data', 'copy', 'keep', 'sync', '•if', 'multiple', 'core', 'different', 'thread', 'execute', 'task', 'parallel', 'accessing', 'data', 'instance', 'memory', '15', 'task', 'parallelism', '-example', '2', 'independent', 'statistic', '•given', 'list', 'l', 'numeric', 'value', 'find', 'mean', 'median', 'mode', '•solution', '•independent', 'task', 'data', '•each', 'task', 'find', 'statistic', 'son', 'l', '•run', 'task', 'parallel', '16task', 'parallelism', 'summary', '•identify', 'sub', '-tasks', 'based', 'functionality', 'common', 'function', '•in', 'tree', 'data', 'parallel', 'task', 'identical', 'function', '•sub-tasks', 'identified', 'based', 'data', '•independent', 'sub', '-tasks', 'executed', 'parallel', '•sub-tasks', 'often', 'limited', 'known', 'statically', 'advance', '•we', 'know', 'word', 'processor', 'sub', '-tasks', '•we', 'know', 'statistical', 'analysis', 'function', 'run', 'advance', '•so', 'limited', 'parallelism', 'scope', '-not', 'scalable', 'resource', '•in', 'data', 'tree', 'parallelism', 'potentially', 'get', 'parallelism', 'data', '-more', 'scalable', 'resource', 'time', 'interval', '17request', 'parallelism', '•problem', '•scalable', 'execution', 'independent', 'task', 'parallel', '•execute', 'code', 'many', 'parallel', 'instance', '•solution', '•on', 'arrival', 'request', 'serviced', 'parallel', 'along', 'existing', 'task', 'servicing', 'prior', 'request', '•could', 'processing', 'fixed', 'data', '•request', '-reply', 'pair', 'independent', 'serviced', 'different', 'thread', 'process', 'backend', '•there', 'could', 'application', 'specific', 'backend', 'dependency', 'e.g', 'get', 'post', 'data', 'item', '•systems', 'fit', '•servers', 'client', '-server', 'model', '•e.g', 'email', 'server', 'http', 'web', '-server', 'cloud', 'service', 'api', 'interface', 'file', 'storage', 'server', '•microservices', '•socket', 'programming', '•scalability', 'metric', 'request', 'time', 'throughput', '18what', 'happens', 'loosely', 'coupled', 'distributed', 'system', '•divide', '•no', 'shared', 'memory', '•memory', 'storage', 'separate', 'node', '•so', 'exchange', 'data', 'coordination', 'task', 'via', 'message', 'passing', '•divide', 'problem', 'way', 'computation', 'task', 'run', 'local', 'data', '•conquer', 'merge', '•in', 'shared', 'memory', 'merge', 'simpler', 'process', 'writing', 'memory', 'location', '•in', 'distributed', '•need', 'collect', 'data', 'different', 'node', '•in', 'search', 'example', 'simpler', 'merge', 'collect', 'result', '-so', 'low', 'cost', '•in', 'quick', 'sort', 'simple', 'append', 'whether', 'writing', 'place', 'shared', 'memory', 'sending', 'message', '•sometimes', 'merges', 'may', 'become', 'sequential', '•e.g', 'k', '-means', '-in', 'iteration', 'guess', 'cluster', 'parallel', 'improve', 'cluster', '2', 'checking', 'found', 'right', 'cluster', 'sequential', '20message', 'passing', 'communicationstopics', 'today', '•top', 'design', '•types', 'parallelism', '•mapreduce', 'programming', 'model', '•see', 'map', 'reduce', 'program', 'work', 'using', 'hadoop', '•iterative', 'mapreduce', '21mapreduce', 'origin', '•created', 'google', 'gfs', '•open', 'source', 'version', 'created', 'apache', 'hadoop', '•perform', 'maps/reduces', 'data', 'using', 'many', 'machine', '▪the', 'system', 'take', 'care', 'distributing', 'data', 'managing', 'fault', 'tolerance', '▪you', 'write', 'code', 'map', 'one', 'element', 'reduce', 'element', 'combined', 'result', '•separates', 'recursive', 'divide', '-and-conquer', 'computation', 'perform', '▪old', 'idea', 'higher', '-order', 'functional', 'programming', 'transferred', 'large', '-scale', 'distributed', 'computing', '▪complementary', 'approach', 'database', 'declarative', 'query', '▪in', 'sql', '’', 'actually', 'write', 'low', '-level', 'query', 'execution', 'code', '▪programmer', 'need', 'focus', 'map', 'reduce', 'logic', 'rest', 'work', 'done', 'map', '-reduce', 'framework', '▪so', 'restricted', 'programming', 'interface', 'system', 'let', 'system', 'distribution', 'work', 'job', 'tracking', 'fault', 'tolerance', 'etc', '22mapreduce', 'evolution', '⚫we', 'large', 'text', 'file', 'word', 'one', 'word', 'line', '⚫we', 'need', 'count', 'number', 'time', 'distinct', 'word', 'appears', 'file', '⚫sample', 'application', 'analyze', 'web', 'server', 'log', 'find', 'popular', 'url', '⚫word', 'count', 'solution', 'design', 'option', '1', 'entire', 'file', 'fit', 'memory', '-read', 'file', 'memory', 'count', '2', 'file', 'large', 'memory', 'word', 'count', 'pair', 'fit', 'memory', '-read', 'file', 'line', 'line', 'maintain', 'word', 'count', 'memory', '3', 'file', 'disk', 'many', 'distinct', 'word', 'fit', 'memory', 'sort', 'words.txt', 'uniq–c', '23solution', 'multiple', 'large', 'file', 'disk', 'make', 'slightly', 'harder', 'suppose', 'large', 'corpus', 'document', 'count', 'number', 'time', 'distinct', 'word', 'occurs', 'corpus', '•words', 'docs/', 'sort', 'uniq–c', 'word', 'take', 'file', 'folder', 'output', 'word', 'one', 'line', '•the', 'capture', 'essence', 'mapreduce', '•great', 'thing', 'naturally', 'parallelizable', '24mapreduce', 'conceptually', 'like', 'unix', 'pipeline', '-one', 'function', 'map', 'process', 'data', '-output', 'input', 'another', 'function', 'reduce', 'cat', 'words.txt', 'sort', 'uniq-c', 'cat', 'file', 'input', 'map', 'shuffle', 'reduce', 'output→', 'developer', 'specifies', 'two', 'function', '-map', '-user', 'code', '-reduce', '-user', 'code', '→rest', 'job', 'done', 'mapreduce', 'framework', '→tune', 'configuration', 'parameter', 'mapreduce', 'framework', 'performancemapreduce', 'term', 'data', 'tree', 'parallelism', '•map', '•data', 'parallelism', '•divide', 'problem', 'sub', '-problems', 'based', 'data', '•reduce', '•inverse', 'tree', 'parallelism', '•with', 'every', 'merge', 'reduce', 'parallelism', 'reduces', 'get', 'one', 'result', '•depending', 'problem', '“', 'reduce', '”', 'step', 'may', 'simple', 'sequential', '25', 'example', 'word', 'count', 'using', 'mapreduce', 'map', 'key', 'value', '//', 'key', 'document', 'name', 'value', 'text', 'document', 'word', 'w', 'value', 'emit', 'w', '1', '26reduce', 'key', 'value', '//', 'key', 'word', 'value', 'iterator', 'count', 'result', '0', 'count', 'v', 'value', 'result', '+=', 'v', 'emit', 'result', 'd1', 'blue', 'ship', 'blue', 'sea', '1', 'blue', '1blue', '1', 'ship', '1', '1', 'sea', '1', '1', 'ship', '1', '1', 'sea', '1', '1', 'blue', '2', 'ship', '1', '1', 'sea', '1', 'sort', 'done', 'key', 'k', 'v', 'k', 'value', 'together', 'blue', '1,1', 'word', 'count', '2', '27', 'mapreduce', 'map', 'step', '28v2k2k', 'v', 'k', 'vmap', 'v1k1', 'vnkn…k', 'vmapinput', 'key-value', 'pairsintermediate', 'key-value', 'pair', '…', 'k', 'v', 'adapted', 'jeff', 'ullman', '’', 'course', 'slidese.g', 'doc', '—id', 'doc', '-content', 'e.g', 'word', 'wordcount', '-in-a-doc', 'mapreduce', 'reduce', 'step', '29k', 'v…k', 'v', 'k', 'v', 'k', 'vintermediate', 'key-value', 'pair', 'group', 'reducek', 'v', 'v', 'v…', 'k', 'v…k', 'v', 'k', 'vvv', 'vkey-value', 'groupsoutput', 'key-value', 'pair', 'adapted', 'jeff', 'ullman', '’', 'course', 'slidese.g', 'word', 'wordcount', '-in-a-doc', 'word', 'list', '-of-wordcount', 'word', 'final', '-count', 'sql', 'group', 'sql', 'aggregationk', 'kformal', 'definition', 'mapreduce', 'program', '•input', 'set', 'key/value', 'pair', '•user', 'supply', 'two', 'function', '•map', 'k', 'v', '→list', 'k1', 'v1', '•reduce', 'k1', 'list', 'v1', '→v2', '•', 'k1', 'v1', 'intermediate', 'key/value', 'pair', '•output', 'set', 'k1', 'v2', 'pair', '30', 'use', '•huge', 'set', 'document', '’', 'fit', 'memory', '•so', 'need', 'file', 'based', 'processing', 'stage', 'e.g', 'hadoop', '•but', 'also', 'memory', '-e.g', 'spark', '•lot', 'data', 'partitioning', 'high', 'data', 'parallelism', '•possibly', 'simple', 'merge', 'among', 'partition', 'low', 'cost', 'inverse', 'tree', 'parallelism', '3132', 'mapreduce', 'execution', 'overview', '•intermediate', 'result', 'disk', '•dynamic', 'task', 'scheduling', '•data', 'centric', 'design', '•move', 'computation', 'closer', 'data', 'mapreduce', 'library', 'runtime', 'work', '-allocating', 'resource', '-starting', 'worker', '-managing', '-moving', 'data', '-handling', 'failure', '…topics', 'today', '•top', 'design', '•types', 'parallelism', '•mapreduce', 'programming', 'model', '•see', 'map', 'reduce', 'program', 'work', 'using', 'hadoop', '•iterative', 'mapreduce', '33mapreduce', 'example', '-sales', 'data', 'processing', '34https', '//www.guru99.com/create', '-your', '-first-hadoop', '-program.html', 'count', 'tx', 'countryunravelling', 'mapreduce', 'program', '-driver', '1', 'package', 'salescountry', 'import', 'org.apache.hadoop.fs.path', 'import', 'org.apache.hadoop.io', 'import', 'org.apache.hadoop.mapred', 'public', 'class', 'salescountrydriver', 'public', 'static', 'void', 'main', 'string', 'args', 'jobclient', 'my_client', 'new', 'jobclient', '//', 'create', 'configuration', 'object', 'job', 'jobconf', 'job_conf', 'new', 'jobconf', 'salescountrydriver.class', '//', 'set', 'name', 'job', 'job_conf.setjobname', '``', 'salepercountry', '``', '//', 'specify', 'data', 'type', 'output', 'key', 'value', 'job_conf.setoutputkeyclass', 'text.class', 'job_conf.setoutputvalueclass', 'intwritable.class', '35package', 'name', 'jar', 'hadoop', 'libs', 'driver', 'class', 'contains', 'main', 'hadoop', 'job', 'driver', 'class', 'salespercountry', 'application', 'name', 'output', 'data', 'type', 'defined', 'based', 'existing', 'hadoop', 'classesunravelling', 'mapreduce', 'program', '-driver', '2', '//', 'specify', 'name', 'mapper', 'reducer', 'class', 'job_conf.setmapperclass', 'salescountry.salesmapper.class', 'job_conf.setreducerclass', 'salescountry.salescountryreducer.class', '//', 'set', 'input', 'output', 'directory', '//', 'arg', '0', 'name', 'input', 'directory', 'hdfs', '//', 'arg', '1', 'name', 'output', 'directory', 'created', '//', 'store', 'output', 'file', 'fileinputformat.setinputpaths', 'job_conf', 'new', 'path', 'args', '0', 'fileoutputformat.setoutputpath', 'job_conf', 'new', 'path', 'args', '1', 'my_client.setconf', 'job_conf', 'try', '//', 'run', 'job', 'jobclient.runjob', 'job_conf', 'catch', 'exception', 'e', 'e.printstacktrace', '36paths', 'read', 'input', 'send', 'output', 'send', 'job', 'executionmapper', 'reducer', 'class', 'jar', 'pkgunravelling', 'mapreduce', 'program', '-mapper', 'package', 'salescountry', 'import', 'java.io.ioexception', 'import', 'org.apache.hadoop.io.intwritable', 'import', 'org.apache.hadoop.io.longwritable', 'import', 'org.apache.hadoop.io.text', 'import', 'org.apache.hadoop.mapred', 'public', 'class', 'salesmapper', 'extends', 'mapreducebase', 'implement', 'mapper', 'longwritable', 'text', 'text', 'intwritable', 'private', 'final', 'static', 'intwritable', 'one', 'new', 'intwritable', '1', 'public', 'void', 'map', 'longwritable', 'key', 'text', 'value', 'outputcollector', 'text', 'intwritable', 'output', 'reporter', 'reporter', 'throw', 'ioexception', 'string', 'valuestring', 'value.tostring', 'string', 'singlecountrydata', 'valuestring.split', '``', \"''\", 'output.collect', 'new', 'text', 'singlecountrydata', '7', 'one', '37map', 'logichadoop', 'libsapp', 'jar', 'package', 'map', 'function', 'w', 'input', '-one', 'key', 'value', 'pair', '-output', 'collector', '-…', 'india', '1', 'usa', '1', 'india', '1', '….//', 'add', 'data', 'structure', 'across', 'map', 'instancesunravelling', 'mapreduce', 'program', '-reducer', 'package', 'salescountry', 'import', 'java.io.ioexception', 'import', 'java.util', 'import', 'org.apache.hadoop.io.intwritable', 'import', 'org.apache.hadoop.io.text', 'import', 'org.apache.hadoop.mapred', 'public', 'class', 'salescountryreducer', 'extends', 'mapreducebase', 'implement', 'reducer', 'text', 'intwritable', 'text', 'intwritable', 'public', 'void', 'reduce', 'text', 't_key', 'iterator', 'intwritable', 'value', 'outputcollector', 'text', 'intwritable', 'output', 'reporter', 'reporter', 'throw', 'ioexception', 'text', 'key', 't_key', 'int', 'frequencyforcountry', '0', 'values.hasnext', '//', 'replace', 'type', 'value', 'actual', 'type', 'value', 'intwritable', 'value', 'intwritable', 'values.next', 'frequencyforcountry', '+=', 'value.get', 'output.collect', 'key', 'new', 'intwritable', 'frequencyforcountry', '38reduce', 'logic', '-calculate', 'frequencyreduce', 'function', 'w', 'input', '-key', 'value', 'list', 'e.g', 'india', '1,1,1,1', '-output', 'collector', '-…pkg', 'includes', 'reduce', 'task', '1', 'output', 'file', 'created', 'control', 'reducer', 'driver//', 'add', 'data', 'structure', 'across', 'reduce', 'instancesrunning', 'checking', 'status', '39', 'mapreduce', 'stats', '40', 'topic', 'today', '•top', 'design', '•types', 'parallelism', '•mapreduce', 'programming', 'model', '•see', 'map', 'reduce', 'program', 'work', 'using', 'hadoop', '•k-means', 'clustering', '•iterative', 'mapreduce', '41example', '1', 'k', '-means', 'clustering', '42', 'http', '//shabal.in/visuals/kmeans/2.htmliterative', 'map', 'reduce', '•mapreduce', 'one', '-pass', 'computation', '•many', 'application', 'esp', 'ml', 'data', 'mining', 'area', 'need', 'iteratively', 'process', 'data', '•so', 'need', 'iterative', 'execution', 'map', 'reduce', 'job', '•an', 'approach', 'create', 'main', 'program', 'call', 'core', 'map', 'reduce', 'variable', 'data', '•core', 'program', 'also', 'check', 'convergence', '•error', 'bound', 'e.g', 'k', '-means', 'clustering', '•fixed', 'iteration', '43', 'k-means', 'iterative', 'map', 'reduce', '•the', 'mapreduce', 'program', 'driver', 'responsible', 'repeating', 'step', 'via', 'iterative', 'construct', '•within', 'iteration', 'map', 'reduce', 'step', 'called', '•each', 'map', 'step', 'reuses', 'result', 'produced', 'previous', 'reduce', 'step', '•e.g', 'k', 'center', 'computed', '44', 'http', '//github.com/thomasjungblut/mapreduce', '-kmeans/tree/master/src/de/jungblut/clustering/mapreducek-means', 'clustering', 'iterative', 'mapreduce', '45hands', 'demo', '•20newsgroups', 'folder', '-a', 'set', 'around', '20,000', 'posting', '20', 'different', 'newsgroups', '1000', 'posting', '•convert', 'newsgroup', 'posting', 'turn', 'bag', '-of-words', 'vector', '–/data', 'folder', 'hdfs', '•run', 'program', 'choose', 'initial', 'set', 'cluster', 'centroid', 'data', '–/clusters', 'folder', 'hdfs', 'randomly', 'sampled', 'vector', '•run', 'kmeans', 'hadoop', '-hadoop', 'jar', 'mapredkmeans.jar', 'kmeans', '/data', '/clusters', '3', 'run', '3', 'iteration', 'kmeans', 'algorithm', 'top', 'document', '20_newsgroups', 'data', 'set', 'mean', 'three', 'separate', 'mapreduce', 'job', 'run', 'sequence', '•the', 'centroid', 'produced', 'end', '1.iteration', '1', 'put', 'hdfs', 'directory', '``', '/clusters1', \"''\", '2.iteration', '2', 'put', 'hdfs', 'directory', '``', '/clusters2', \"''\", '3.iteration', '3', 'put', 'hdfs', 'directory', '``', '/clusters3', \"''\", 'reference', 'http', '//cmj4.web.rice.edu/mapredkmeans.htmliterations', 'using', 'existing', 'runtimes', '•loop', 'implemented', 'top', 'existing', 'file-based', 'single', 'step', 'map', '-reduce', 'core', '•large', 'overhead', '•re-initialization', 'task', '•reloading', 'static', 'data', '•communication', 'data', 'transfer', '46', 'distributedcache', 'http', '//hadoop.apache.org/docs/r2.6.3/api/org/apache/hadoop/filecache/distributedcache.htmlmapreduce++', 'iterative', 'mapreduce', '•some', 'optimization', 'done', 'top', 'existing', 'model', '•static', 'data', 'loaded', '•cached', 'task', 'across', 'invocation', '•combine', 'operation', '47', 'example', 'twister', 'enables', 'apis', '48', 'http', '//iterativemapreduce.weebly.com/', 'ref', 'hwang', 'chap', '6', 'pg', '351the', 'optimisation', 'indeed', 'help', '49', 'k-means', 'clustering', 'using', 'various', 'programming', 'modelsiterative', 'mapreduce', 'option', '•haloop', '•modifies', 'hadoop', 'scheduling', 'make', 'loop', 'aware', '•implements', 'cache', 'avoid', 'going', 'disk', 'iteration', '•optional', 'reading', 'paper', 'proceeding', 'vldb', 'endowment', '3', '1', ':285', '-296', 'sep', '2010', '•spark', '•uses', '-memory', 'computing', 'speed', 'iteration', '•an', 'in-memory', 'structure', 'called', 'rdd', 'resilient', 'distributed', 'dataset', 'replaces', 'file', 'disk', '•ideal', 'iterative', 'computation', 'reuse', 'lot', 'data', 'iteration', '50summary', '•different', 'type', 'parallelism', '•data', 'tree', 'parallelism', '—', 'map', 'reduce', '•basics', 'mapreduce', 'program', 'historical', 'sale', 'data', 'processing', 'example', '•optimizations', 'iterative', 'mapreduce', 'requirement', '•how', '-memory', 'computing', 'help', 'iterative', 'mapreduce', 'programming', '51next', 'session', 'hadoop', 'mapreduce', 'yarn']\n",
            "Preprocessed ff.xlsx: ['1', 'health', 'state', 'complete', 'physical', 'mental', 'social', 'well-being', 'merely', 'absence', 'disease', 'infirmity', 'achieving', 'maintaining', 'good', 'health', 'involves', 'combination', 'practice', 'contribute', 'overall', 'well-being', 'include', 'regular', 'physical', 'activity', 'balanced', 'diet', 'adequate', 'rest', 'mental', 'health', 'care', '2', 'nan', '3', 'physical', 'activity', 'essential', 'maintaining', 'healthy', 'body', 'engaging', 'regular', 'exercise', 'help', 'strengthen', 'muscle', 'improve', 'cardiovascular', 'health', 'maintain', 'healthy', 'weight', 'also', 'release', 'endorphin', 'improve', 'mood', 'reduce', 'stress', 'activity', 'walking', 'running', 'cycling', 'swimming', 'excellent', 'way', 'stay', 'active', '4', 'nan', '5', 'balanced', 'diet', 'another', 'cornerstone', 'good', 'health', 'consuming', 'variety', 'food', 'ensures', 'body', 'get', 'essential', 'nutrient', 'vitamin', 'mineral', 'protein', 'carbohydrate', 'eating', 'plenty', 'fruit', 'vegetable', 'whole', 'grain', 'lean', 'protein', 'limiting', 'sugar', 'salt', 'unhealthy', 'fat', 'help', 'prevent', 'chronic', 'disease', 'like', 'diabetes', 'heart', 'disease', 'obesity', '6', 'nan', '7', 'adequate', 'rest', 'also', 'crucial', 'health', 'sleep', 'allows', 'body', 'repair', 'support', 'brain', 'function', 'adult', 'need', '7', '9', 'hour', 'sleep', 'per', 'night', 'function', 'optimally', 'poor', 'sleep', 'lead', 'variety', 'health', 'issue', 'including', 'weakened', 'immunity', 'weight', 'gain', 'impaired', 'cognitive', 'function', '8', 'nan', '9', 'mental', 'health', 'equally', 'important', 'physical', 'health', 'managing', 'stress', 'mindfulness', 'meditation', 'counseling', 'improve', 'mental', 'well-being', 'social', 'connection', 'supportive', 'relationship', 'also', 'play', 'vital', 'role', 'maintaining', 'mental', 'health', '10', 'nan', '11', 'summary', 'achieving', 'good', 'health', 'holistic', 'process', 'requires', 'attention', 'physical', 'activity', 'nutrition', 'rest', 'mental', 'well-being', 'integrating', 'practice', 'daily', 'life', 'individual', 'enhance', 'quality', 'life', 'prevent', 'variety', 'health', 'problem']\n",
            "Preprocessed Upcoming 5G.docx: ['5g', 'network', 'impact', 'advent', '5g', 'technology', 'mark', 'significant', 'milestone', 'evolution', 'mobile', 'network', 'unlike', 'predecessor', '5g', 'promise', 'incremental', 'improvement', 'transformative', 'change', 'impact', 'various', 'sector', 'one', 'profound', 'impact', '5g', 'ability', 'provide', 'ultra-fast', 'internet', 'speed', 'theoretical', 'maximum', 'speed', 'reaching', '20', 'gbps', 'speed', 'enhancement', 'crucial', 'supporting', 'high-definition', 'video', 'streaming', 'virtual', 'reality', 'bandwidth-intensive', 'application', 'addition', 'speed', '5g', 'significantly', 'reduces', 'latency', 'delay', 'transfer', 'data', 'begin', 'following', 'instruction', 'transfer', 'latency', 'low', '1', 'millisecond', '5g', 'enables', 'real-time', 'communication', 'device', 'low', 'latency', 'essential', 'application', 'autonomous', 'vehicle', 'remote', 'surgery', 'industrial', 'automation', 'even', 'slight', 'delay', 'critical', 'consequence', 'another', 'critical', 'impact', '5g', 'ability', 'connect', 'vast', 'number', 'device', 'simultaneously', 'capability', 'support', 'growth', 'internet', 'thing', 'iot', 'everyday', 'object', 'household', 'appliance', 'industrial', 'machine', 'interconnected', 'communicate', 'connectivity', 'pave', 'way', 'smart', 'city', 'infrastructure', 'like', 'traffic', 'light', 'waste', 'management', 'system', 'energy', 'grid', 'operate', 'efficiently', 'constant', 'data', 'exchange', 'moreover', '5g', 'enhance', 'remote', 'working', 'learning', 'experience', 'providing', 'reliable', 'high-speed', 'internet', 'area', 'previously', 'suffered', 'poor', 'connectivity', 'improvement', 'particularly', 'relevant', 'context', 'covid-19', 'pandemic', 'accelerated', 'adoption', 'remote', 'work', 'online', 'education', 'despite', 'benefit', 'rollout', '5g', 'also', 'face', 'challenge', 'need', 'extensive', 'infrastructure', 'upgrade', 'including', 'installation', 'numerous', 'small', 'cell', 'tower', 'requires', 'significant', 'investment', 'additionally', 'concern', 'cybersecurity', 'potential', 'health', 'impact', 'need', 'addressed', 'ensure', 'widespread', 'acceptance', 'adoption', '5g', 'technology', 'overall', '5g', 'poised', 'revolutionize', 'connect', 'communicate', 'operate', 'various', 'facet', 'life']\n",
            "Preprocessed cloud.txt: ['cloud', 'computing', 'service', 'overview', 'cloud', 'computing', 'service', 'revolutionized', 'way', 'business', 'individual', 'manage', 'store', 'process', 'data', 'leveraging', 'power', 'internet', 'cloud', 'computing', 'provides', 'on-demand', 'access', 'shared', 'pool', 'computing', 'resource', 'server', 'storage', 'application', 'service', 'resource', 'rapidly', 'provisioned', 'released', 'minimal', 'management', 'effort', 'offering', 'flexibility', 'efficiency', 'traditional', 'computing', 'model', 'match', 'one', 'primary', 'advantage', 'cloud', 'computing', 'scalability', 'business', 'scale', 'resource', 'based', 'demand', 'avoiding', 'need', 'significant', 'upfront', 'investment', 'hardware', 'software', 'scalability', 'particularly', 'beneficial', 'startup', 'small', 'medium-sized', 'enterprise', 'smes', 'need', 'adapt', 'quickly', 'changing', 'market', 'condition', 'without', 'incurring', 'substantial', 'cost', 'cloud', 'computing', 'service', 'typically', 'categorized', 'three', 'main', 'type', 'infrastructure', 'service', 'iaa', 'platform', 'service', 'paas', 'software', 'service', 'saas', 'iaa', 'provides', 'virtualized', 'computing', 'resource', 'internet', 'allowing', 'business', 'rent', 'infrastructure', 'like', 'server', 'storage', 'paas', 'offer', 'platform', 'allowing', 'customer', 'develop', 'run', 'manage', 'application', 'without', 'complexity', 'building', 'maintaining', 'underlying', 'infrastructure', 'saas', 'delivers', 'software', 'application', 'internet', 'subscription', 'basis', 'enabling', 'user', 'access', 'program', 'device', 'internet', 'connection', 'another', 'significant', 'benefit', 'cloud', 'computing', 'support', 'remote', 'work', 'collaboration', 'cloud-based', 'tool', 'application', 'enable', 'employee', 'access', 'file', 'communicate', 'collaborate', 'colleague', 'anywhere', 'world', 'capability', 'become', 'increasingly', 'important', 'wake', 'covid-19', 'pandemic', 'accelerated', 'shift', 'towards', 'remote', 'hybrid', 'working', 'model', 'security', 'data', 'privacy', 'critical', 'consideration', 'cloud', 'computing', 'reputable', 'cloud', 'service', 'provider', 'implement', 'robust', 'security', 'measure', 'including', 'encryption', 'identity', 'access', 'management', 'regular', 'security', 'audit', 'protect', 'sensitive', 'data', 'however', 'business', 'must', 'also', 'adopt', 'best', 'practice', 'regular', 'backup', 'strong', 'password', 'safeguard', 'data', 'overall', 'cloud', 'computing', 'service', 'offer', 'flexible', 'scalable', 'cost-effective', 'solution', 'modern', 'computing', 'need', 'transforming', 'business', 'operate', 'innovate', 'digital', 'age']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boolean Query Function"
      ],
      "metadata": {
        "id": "joPvqBb5FtzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_query(query, inverted_index, doc_id_mapping):\n",
        "    query = query.lower()\n",
        "    query_tokens = word_tokenize(query)\n",
        "    result = None\n",
        "    all_docs = set(doc_id_mapping.keys())\n",
        "\n",
        "    i = 0\n",
        "    while i < len(query_tokens):\n",
        "        token = query_tokens[i]\n",
        "\n",
        "        if token == 'not':\n",
        "            i += 1\n",
        "            next_token = query_tokens[i] if i < len(query_tokens) else None\n",
        "            if next_token:\n",
        "                result = result.difference(set(inverted_index.get(next_token, {}).keys())) if result is not None else all_docs.difference(set(inverted_index.get(next_token, {}).keys()))\n",
        "        elif token in ('and', 'or'):\n",
        "            i += 1\n",
        "            continue\n",
        "        else:\n",
        "            if result is None:\n",
        "                result = set(inverted_index.get(token, {}).keys())\n",
        "            elif query_tokens[i-1] == 'and':\n",
        "                result = result.intersection(set(inverted_index.get(token, {}).keys()))\n",
        "            elif query_tokens[i-1] == 'or':\n",
        "                result = result.union(set(inverted_index.get(token, {}).keys()))\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return list(result) if result is not None else []"
      ],
      "metadata": {
        "id": "fldZvGPoFGjN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Function"
      ],
      "metadata": {
        "id": "m5gg7TZjGb4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct inverted index\n",
        "documents_path = '/content/Documents'\n",
        "inverted_index, doc_id_mapping = construct_inverted_index(documents_path)\n",
        "print (doc_id_mapping)\n",
        "\n",
        "# Perform boolean queries\n",
        "query1 = \"context AND program\"\n",
        "query2 = \"Health OR activity\"\n",
        "query3 = \"NOT health\"\n",
        "\n",
        "result1 = boolean_query(query1, inverted_index, doc_id_mapping)\n",
        "result2 = boolean_query(query2, inverted_index, doc_id_mapping)\n",
        "result3 = boolean_query(query3, inverted_index, doc_id_mapping)\n",
        "\n",
        "print(f\"Results for query '{query1}': {[doc_id_mapping[doc_id] for doc_id in result1]}\")\n",
        "print(f\"Results for query '{query2}': {[doc_id_mapping[doc_id] for doc_id in result2]}\")\n",
        "print(f\"Results for query '{query3}': {[doc_id_mapping[doc_id] for doc_id in result3]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fogj9ri3Gd4-",
        "outputId": "c766e95c-b3a7-4793-b6f7-099488379a71"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'doc_1': 'dd.pdf', 'doc_2': 'ff.xlsx', 'doc_3': 'Upcoming 5G.docx', 'doc_4': 'cloud.txt'}\n",
            "Results for query 'context AND program': ['dd.pdf']\n",
            "Results for query 'Health OR activity': ['ff.xlsx', 'Upcoming 5G.docx', 'dd.pdf']\n",
            "Results for query 'NOT health': ['cloud.txt', 'dd.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation and analysis of Context sensitive spell correction algorithm"
      ],
      "metadata": {
        "id": "SDv3EUW0OkOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the input text: tokenization, lowercasing, removing stopwords and punctuation.\n",
        "    Returns a list of preprocessed tokens.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "    return tokens\n",
        "\n",
        "def is_misspelled(word, vocabulary):\n",
        "    \"\"\"\n",
        "    Checks if a word is likely misspelled based on vocabulary.\n",
        "    \"\"\"\n",
        "    return word.lower() not in vocabulary\n",
        "\n",
        "def generate_candidates(word, vocabulary):\n",
        "    \"\"\"\n",
        "    Generates candidate corrections for a misspelled word.\n",
        "    Uses edit distance and dictionary lookup.\n",
        "    \"\"\"\n",
        "    # Generate candidates using edit distance\n",
        "    candidates = []\n",
        "    for vocab_word in vocabulary:\n",
        "        if nltk.edit_distance(word, vocab_word) <= 1:\n",
        "            candidates.append(vocab_word)\n",
        "\n",
        "    # Add the original word as a candidate\n",
        "    candidates.append(word)\n",
        "\n",
        "    return candidates\n",
        "\n",
        "def rank_candidates(original_word, candidates, context_tokens):\n",
        "    \"\"\"\n",
        "    Ranks candidate corrections based on context.\n",
        "    Enhanced ranking by considering POS tags and edit distance.\n",
        "    \"\"\"\n",
        "    candidate_scores = defaultdict(int)\n",
        "\n",
        "    # Count frequency of each candidate in the context tokens\n",
        "    for candidate in candidates:\n",
        "        candidate_scores[candidate] += context_tokens.count(candidate)\n",
        "\n",
        "    # Rank candidates by frequency (higher frequency is better)\n",
        "    ranked_candidates = sorted(candidates, key=lambda x: candidate_scores[x], reverse=True)\n",
        "\n",
        "    # Return the best candidate (first in the list)\n",
        "    return ranked_candidates[0]\n",
        "\n",
        "def context_sensitive_spell_correction(text, vocabulary):\n",
        "    \"\"\"\n",
        "    Performs context-sensitive spell correction on the input text.\n",
        "    Returns the corrected text.\n",
        "    \"\"\"\n",
        "    # Preprocess the text\n",
        "    tokens = preprocess_text(text)\n",
        "\n",
        "    # POS tagging\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Context-sensitive correction logic goes here\n",
        "    corrected_text = []\n",
        "    for i, (token, tag) in enumerate(tagged_tokens):\n",
        "        # Check if the token is likely misspelled\n",
        "        if is_misspelled(token, vocabulary):\n",
        "            # Generate candidate corrections\n",
        "            candidates = generate_candidates(token, vocabulary)\n",
        "\n",
        "            # Rank candidates based on context (for simplicity, consider only surrounding tokens)\n",
        "            context_tokens = tokens[max(0, i-2):i] + tokens[i+1:min(len(tokens), i+3)]\n",
        "            best_candidate = rank_candidates(token, candidates, context_tokens)\n",
        "\n",
        "            # Replace token with best candidate\n",
        "            corrected_text.append(best_candidate)\n",
        "        else:\n",
        "            corrected_text.append(token)\n",
        "\n",
        "    # Join tokens back into corrected text\n",
        "    corrected_text = ' '.join(corrected_text)\n",
        "    return corrected_text\n",
        "\n",
        "# Example vocabulary (can be expanded with a larger corpus)\n",
        "vocabulary = {'this', 'business', 'analysis', 'important', 'for', 'making', 'decisions'}\n",
        "\n",
        "# Example usage\n",
        "input_text = \"Ths busi analysi is importnt for makin decisins.\"\n",
        "corrected_text = context_sensitive_spell_correction(input_text, vocabulary)\n",
        "print(\"Original Text:\", input_text)\n",
        "print(\"Corrected Text:\", corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU-XUuAfOs6z",
        "outputId": "2dfe5535-438a-403c-a5a3-b04ea836b447"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Ths busi analysi is importnt for makin decisins.\n",
            "Corrected Text: this busi analysis important making decisions\n"
          ]
        }
      ]
    }
  ]
}